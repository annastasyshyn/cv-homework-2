{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab42910-dabd-4f17-b552-d09fa0a616b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/NVlabs/nvdiffrast /tmp/pip-req-build-fbedh8bz\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy >> /dev/null\n",
    "! pip install pandas >> /dev/null\n",
    "! pip install opencv-python >> /dev/null\n",
    "! pip install scipy >> /dev/null\n",
    "! pip install torch >> /dev/null\n",
    "! pip install matplotlib >> /dev/null\n",
    "! pip install git+https://github.com/NVlabs/nvdiffrast >> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb91bcb-3ae4-4176-8b5b-3d9d3ea18717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff353fa-1760-4501-acd2-f2b5f4e5ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"data\" # replace with your sequence\n",
    "ORIGINAL_VIDEONAME = \"output/orig.mp4\"\n",
    "UNDISTORTED_VIDEONAME = \"output/undist.mp4\"\n",
    "RS_REMOVED_VIDEONAME = \"output/nors.mp4\"\n",
    "STABILIZED_VIDEONAME = \"output/stable_nors.mp4\"\n",
    "\n",
    "# camera params\n",
    "FPS = 20\n",
    "\n",
    "# camera intrinsics\n",
    "# K = [739.1654756101043, 739.1438452683457, 625.826167006398, 517.3370973594253]\n",
    "K = np.array([[739.1654756101043, 0.0, 625.826167006398],\n",
    "              [0.0, 739.1438452683457, 517.3370973594253],\n",
    "              [0.0, 0.0, 1.0]])\n",
    "\n",
    "# output camera intrinsics\n",
    "K_new = K # pominyatu potim\n",
    "\n",
    "# distortion params\n",
    "D = np.array([0.019327620961435945, 0.006784242994724914, -0.008658628531456217, 0.0051893686731546585])\n",
    "\n",
    "T_cam_imu = np.array([[-0.0027687291789002095, -0.9999872674970001, 0.004218884048773266, -0.05688384754602901],\n",
    "                       [-0.9999356528558058, 0.002814949729190873, 0.010989367855138411, 0.007618902284014501],\n",
    "                       [-0.011001103879489809, -0.004188185992194848, -0.9999307150055582, -0.042436390295094266],\n",
    "                       [0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "R_cam_imu = T_cam_imu[:3, :3]\n",
    "\n",
    "\n",
    "dt_rs = 29.47 * 10 ** -6  # in seconds\n",
    "\n",
    "# frame dimensions\n",
    "W, H = 1280, 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e614ca20-f87a-40dd-a8af-1ded4446154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames in the video: 808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing frames to video: 100%|█████████████████████████████████████████████████████████████████████████████████| 808/808 [00:06<00:00, 133.14it/s]\n"
     ]
    }
   ],
   "source": [
    "def frames_to_video(frames, output_video, fps=20, isColor=False):\n",
    "    \"\"\"\n",
    "    Create a video from a list of frames.\n",
    "\n",
    "    :param frames: list of frames\n",
    "    :param output_video: output video file name (e.g., 'output.mp4')\n",
    "    :param fps: frames per second (default=20)\n",
    "    \"\"\"\n",
    "    if len(frames) == 0:\n",
    "        raise ValueError(\"No frames provided\")\n",
    "\n",
    "    # Use the first frame to get dimensions\n",
    "    height, width, *channels = frames[0].shape\n",
    "\n",
    "    # Define video writer (using mp4v codec)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_video, fourcc, fps, (width, height), isColor=isColor)\n",
    "\n",
    "    for frame in tqdm(frames, desc=\"Writing frames to video: \"):\n",
    "        video.write(frame)\n",
    "        \n",
    "\n",
    "frames_paths = sorted(glob.glob(os.path.join(INPUT_DIR, \"cam1/data/*.png\")))\n",
    "print(f\"Number of frames in the video: {len(frames_paths)}\")\n",
    "\n",
    "frames = [cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE) for frame_path in frames_paths]\n",
    "frames_to_video(frames, ORIGINAL_VIDEONAME, FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa6809-4d6f-4ac8-978f-69c2478b8d4c",
   "metadata": {},
   "source": [
    "### Homework 2\n",
    "\n",
    "Given input video from camera with rolling shutter, implement and apply the following algorithms:\n",
    "\n",
    "    - radial distortion correction (use cv2.fisheye module)\n",
    "    - rolling shutter correction\n",
    "    - video stabilization\n",
    "\n",
    "Expected output of the homework:\n",
    "\n",
    "    - Jupyter notebook with working code that we can run in real-time\n",
    "    - plots of camera orientations in time for unstabilized and stabilized motion [plot individual x/y/z components in axis-angle representation]\n",
    "    - plots of input/output meshgrids for the video frame, where angular velocity has the biggest norm\n",
    "    - 4 videos stacked together with their frames side-by-side:\n",
    "        - original\n",
    "        - undistorted \n",
    "        - undistorted + rolling shutter corrected\n",
    "        - undistorted + rolling shutter corrected + stabilized\n",
    "\n",
    "Note:\n",
    "Dataset used in homework provides two sources of camera orientations: IMU (Gyro) measurements of angular velocity and Motion Capture direct measurements of orientation. You should use IMU (Gyro) measurements and implement angular velocity integration. You can use Motion Capture direct measurements of orientation to verify that your integration is implemented correctly.\n",
    "\n",
    "Rolling shutter correction algorithm is based on this paper: [Digital Video Stabilization and Rolling Shutter Correction using Gyroscopes](https://graphics.stanford.edu/papers/stabilization/).\n",
    "\n",
    "Dataset: [Rolling-Shutter Visual-Inertial Odometry Dataset](https://cvg.cit.tum.de/data/datasets/rolling-shutter-dataset)\n",
    "\n",
    "Calibrated camera params can be found [here](https://cdn3.vision.in.tum.de/rolling/calibration/camchain-calibration-equidistant4_camimu_dataset-calib-imu1.yaml)\n",
    "\n",
    "![Camera Set-up](https://cvg.cit.tum.de/_media/data/datasets/rolling-shutter-dataset/sensor_axes_rgb.jpg?w=800&tok=fa3d4a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f723756",
   "metadata": {},
   "source": [
    "## Preprocessing IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2edb933-ded6-4167-9f1b-f0c38158d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_data = pd.read_csv(f\"{INPUT_DIR}/imu0/data.csv\", header=None, dtype=np.float64, skiprows=1)\n",
    "imu_data.columns = [\"timestamp\", \"w_x\", \"w_y\", \"w_z\", \"a_x\", \"a_y\", \"a_z\"]\n",
    "gyro_data = imu_data[[\"timestamp\", \"w_x\", \"w_y\", \"w_z\"]].to_numpy()\n",
    "accel_data = imu_data[[\"timestamp\", \"a_x\", \"a_y\", \"a_z\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d875c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_gyro(gyro):\n",
    "    dt = np.diff(gyro[:, 0], prepend=gyro[0, 0])\n",
    "    omega = gyro[:, 1:4]\n",
    "    rotations = []\n",
    "\n",
    "    R_curr = R.from_quat([1,0,0,0])\n",
    "    for w, dt_ in tqdm(zip(omega, dt)):\n",
    "        angle = np.linalg.norm(w * dt_)\n",
    "        if angle > 0:\n",
    "            axis = w / np.linalg.norm(w)\n",
    "            dR = R.from_rotvec(axis * angle)\n",
    "        else:\n",
    "            dR = R.from_quat([1,0,0,0])\n",
    "        R_curr = R_curr * dR\n",
    "        rotations.append(R_curr.as_matrix())\n",
    "    return np.array(rotations)\n",
    "\n",
    "# def integrate_accel(accel):\n",
    "#     dt = accel[1:, 0] - accel[:-1, 0]\n",
    "#     dt = np.concatenate(([dt[0]], dt))\n",
    "#     a = accel[:, 1:4]\n",
    "#     v = np.cumsum(a * dt[:, np.newaxis], axis=0)\n",
    "#     p = np.cumsum(v * dt[:, np.newaxis], axis=0)\n",
    "#     return v, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa30dee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7647it [00:00, 52891.52it/s]\n"
     ]
    }
   ],
   "source": [
    "rotaions = integrate_gyro(gyro_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c11fd9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMU_fs = 200\n",
    "imu_to_fps_ratio = IMU_fs // FPS\n",
    "rotaions_fps = rotaions[::imu_to_fps_ratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49d512d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.          0.          0.        ]\n",
      "  [ 0.          1.          0.        ]\n",
      "  [ 0.          0.          1.        ]]\n",
      "\n",
      " [[ 0.42585699 -0.16231187 -0.89011273]\n",
      "  [ 0.49249711  0.86685164  0.07755537]\n",
      "  [ 0.75900753 -0.47140544  0.44909296]]\n",
      "\n",
      " [[ 0.99996988  0.00695554 -0.00344405]\n",
      "  [-0.00148033 -0.26467306 -0.96433707]\n",
      "  [-0.00761903  0.96431312 -0.2646548 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.01978024 -0.97477925  0.22229295]\n",
      "  [ 0.25674921 -0.21983274 -0.94114473]\n",
      "  [ 0.96627563  0.03845747  0.25462214]]\n",
      "\n",
      " [[-0.30856201 -0.87158569 -0.38095651]\n",
      "  [-0.24700936  0.46018055 -0.85277209]\n",
      "  [ 0.91857273 -0.16903325 -0.35728406]]\n",
      "\n",
      " [[ 0.04116155  0.90461274 -0.42424228]\n",
      "  [ 0.95303215 -0.16306041 -0.25522738]\n",
      "  [-0.30005906 -0.39381098 -0.86883685]]]\n"
     ]
    }
   ],
   "source": [
    "print(rotaions_fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f41774",
   "metadata": {},
   "source": [
    "## Undistort frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "undistorted_frames = [cv2.fisheye.undistortImage(image, K, D, None, K) for image in tqdm(frames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_to_video(undistorted_frames, UNDISTORTED_VIDEONAME, FPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
